"""
Apply module for processing .apl files and updating Topdesk assets.

This module reads APL (Application Change List) files generated by the TUI,
parses their JSON structure, and applies changes to the Topdesk system
using the topdesk-cli command-line interface.

Features:
- Batch processing for efficiency
- Transaction management with rollback capability
- Comprehensive error handling and recovery
- Progress tracking and reporting
- Dry-run mode for testing
"""

import json
import subprocess
import time
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from collections import defaultdict
import shutil
import tempfile
from contextlib import contextmanager
import base64

# Import the logger if available
try:
    from .logger import MergerLogger
except ImportError:
    # Fallback if logger is not available
    class MergerLogger:
        DEBUG = 10
        INFO = 20
        WARNING = 30
        ERROR = 40
        CRITICAL = 50

        def __init__(self, *args, **kwargs):
            self.agent_name = kwargs.get('agent_name', 'applier')

        def log(self, level, message, **kwargs):
            print(f"[{level}] {message}")
        def error(self, message, **kwargs):
            print(f"[ERROR] {message}")
        def info(self, message, **kwargs):
            print(f"[INFO] {message}")
        def warning(self, message, **kwargs):
            print(f"[WARNING] {message}")
        def debug(self, message, **kwargs):
            print(f"[DEBUG] {message}")


class APLProcessor:
    """Processes APL files and applies changes to Topdesk."""

    def __init__(self,
                 output_dir: str = './output',
                 batch_size: int = 10,
                 max_retries: int = 3,
                 retry_delay: float = 2.0,
                 dry_run: bool = False,
                 verbose: bool = False,
                 parallel: bool = False,
                 parallel_workers: int = 4,
                 topdesk_url: Optional[str] = None,
                 topdesk_username: Optional[str] = None,
                 topdesk_api_key: Optional[str] = None):
        """
        Initialize the APL Processor.

        Args:
            output_dir: Directory for output files and reports
            batch_size: Number of assets to process in a batch
            max_retries: Maximum retry attempts for failed operations
            retry_delay: Initial delay between retries (exponential backoff)
            dry_run: If True, only simulate changes without applying
            verbose: Enable verbose logging
            parallel: Enable parallel processing
            parallel_workers: Number of parallel workers
        """
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.dry_run = dry_run
        self.verbose = verbose
        self.parallel = parallel
        self.parallel_workers = parallel_workers

        # Topdesk authentication
        self.topdesk_url = topdesk_url or os.environ.get('TOPDESK_URL')
        self.topdesk_username = topdesk_username or os.environ.get('TOPDESK_USERNAME')
        self.topdesk_api_key = topdesk_api_key or os.environ.get('TOPDESK_API_KEY')

        # Check if topdesk-cli is available
        self.cli_available = self._check_cli_availability()
        self.use_api_directly = not self.cli_available

        # Initialize logger
        log_level = MergerLogger.DEBUG if verbose else MergerLogger.INFO
        self.logger = MergerLogger(
            output_dir=str(self.output_dir),
            log_file='apply.log',
            log_level=log_level,
            agent_name='applier'
        )

        # Initialize statistics
        self.stats = {
            'total_assets': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'partial_updates': 0,
            'skipped': 0,
            'rollbacks': 0
        }

        # Store rollback information
        self.rollback_data = []

        # Create necessary directories
        self._setup_directories()

        # Validate authentication
        if not self.dry_run:
            self._validate_authentication()

    def _setup_directories(self):
        """Create necessary directories for operation."""
        directories = [
            self.output_dir / 'apply',
            self.output_dir / 'apply' / 'success',
            self.output_dir / 'apply' / 'failed',
            self.output_dir / 'apply' / 'rollback',
            self.output_dir / 'apply' / 'reports'
        ]

        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)

    def _check_cli_availability(self) -> bool:
        """Check if topdesk command is available."""
        try:
            result = subprocess.run(
                ['which', 'topdesk'],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                self.logger.info("topdesk command found at: " + result.stdout.strip())
                return True
        except Exception:
            pass

        self.logger.warning("topdesk command not found in PATH")
        return False

    def _validate_authentication(self):
        """Validate Topdesk authentication."""
        if not self.topdesk_url:
            self.logger.error("TOPDESK_URL environment variable is not set")
            self.logger.info("Please set: export TOPDESK_URL=https://your-instance.topdesk.net")
            raise ValueError("Missing TOPDESK_URL")

        if not self.topdesk_username:
            self.logger.error("TOPDESK_USERNAME environment variable is not set")
            self.logger.info("Please set: export TOPDESK_USERNAME=your-username")
            raise ValueError("Missing TOPDESK_USERNAME")

        if not self.topdesk_api_key:
            self.logger.error("TOPDESK_API_KEY environment variable is not set")
            self.logger.info("Please set: export TOPDESK_API_KEY=your-api-key")
            self.logger.info("You can generate an API key in Topdesk: Settings -> API Management")
            raise ValueError("Missing TOPDESK_API_KEY")

        # Test authentication
        if self.use_api_directly:
            if not self._test_api_connection():
                raise ConnectionError("Failed to authenticate with Topdesk API")
        else:
            if not self._test_cli_connection():
                raise ConnectionError("Failed to authenticate with topdesk-cli")

    def _test_api_connection(self) -> bool:
        """Test direct API connection."""
        try:
            import requests

            # Create basic auth header
            auth_string = f"{self.topdesk_username}:{self.topdesk_api_key}"
            auth_bytes = auth_string.encode('ascii')
            auth_b64 = base64.b64encode(auth_bytes).decode('ascii')

            headers = {
                'Authorization': f'Basic {auth_b64}',
                'Content-Type': 'application/json'
            }

            # Test with a simple API call
            response = requests.get(
                f"{self.topdesk_url}/tas/api/assetmgmt/assets",
                headers=headers,
                params={'pageSize': 1},
                timeout=10
            )

            if response.status_code == 200:
                self.logger.info("Successfully authenticated with Topdesk API")
                return True
            elif response.status_code == 401:
                self.logger.error("Authentication failed: Invalid credentials")
                return False
            else:
                self.logger.error(f"API test failed with status: {response.status_code}")
                return False

        except Exception as e:
            self.logger.error(f"Failed to connect to Topdesk API: {e}")
            return False

    def _test_cli_connection(self) -> bool:
        """Test topdesk command connection."""
        try:
            cmd = self._build_cli_base_command()
            cmd.extend(['assets', 'list', '--limit', '1'])

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=10,
                env=self._get_cli_env()
            )

            if result.returncode == 0:
                self.logger.info("Successfully authenticated with topdesk-cli")
                return True
            else:
                self.logger.error(f"CLI authentication failed: {result.stderr}")
                return False

        except subprocess.TimeoutExpired:
            self.logger.error("CLI connection test timed out")
            return False
        except Exception as e:
            self.logger.error(f"Failed to test CLI connection: {e}")
            return False

    def _get_cli_env(self) -> Dict[str, str]:
        """Get environment variables for CLI commands."""
        env = os.environ.copy()
        if self.topdesk_url:
            env['TOPDESK_URL'] = self.topdesk_url
        if self.topdesk_username:
            env['TOPDESK_USERNAME'] = self.topdesk_username
        if self.topdesk_api_key:
            env['TOPDESK_API_KEY'] = self.topdesk_api_key
        return env

    def _build_cli_base_command(self) -> List[str]:
        """Build base command for topdesk."""
        cmd = ['topdesk']

        # Add authentication flags if not using environment variables
        if self.topdesk_url and '--url' not in cmd:
            cmd.extend(['--url', self.topdesk_url])
        if self.topdesk_username and '--username' not in cmd:
            cmd.extend(['--username', self.topdesk_username])
        if self.topdesk_api_key and '--api-key' not in cmd:
            cmd.extend(['--api-key', self.topdesk_api_key])

        return cmd

    def process_apl_file(self, apl_file_path: str) -> Dict[str, Any]:
        """
        Process a single APL file and apply changes to Topdesk.

        Args:
            apl_file_path: Path to the APL file to process

        Returns:
            Dictionary containing processing results and statistics
        """
        self.logger.info(f"Starting processing of APL file: {apl_file_path}")

        # Reset statistics for this run
        self.stats = {
            'total_assets': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'partial_updates': 0,
            'skipped': 0,
            'rollbacks': 0
        }

        start_time = time.time()

        try:
            # Load and validate APL file
            apl_data = self._load_apl_file(apl_file_path)

            if not apl_data:
                self.logger.error(f"Failed to load APL file: {apl_file_path}")
                return self._create_result(success=False, error="Failed to load APL file")

            # Validate structure
            validation_result = self._validate_apl_data(apl_data)
            if not validation_result['valid']:
                self.logger.error(f"APL validation failed: {validation_result['errors']}")
                return self._create_result(success=False, error=validation_result['errors'])

            self.stats['total_assets'] = len(apl_data)

            # Prepare rollback data
            if not self.dry_run:
                self._prepare_rollback(apl_data)

            # Process assets in batches
            results = self._process_assets_batch(apl_data)

            # Generate report
            report_path = self._generate_report(apl_file_path, results, start_time)

            # Archive processed file
            self._archive_apl_file(apl_file_path, success=self.stats['failed_updates'] == 0)

            return self._create_result(
                success=True,
                report_path=str(report_path),
                stats=self.stats.copy()
            )

        except Exception as e:
            self.logger.error(f"Unexpected error processing APL file: {str(e)}")

            # Attempt rollback if needed
            if not self.dry_run and self.rollback_data:
                self._perform_rollback()

            return self._create_result(success=False, error=str(e))

    def _load_apl_file(self, file_path: str) -> Optional[List[Dict]]:
        """Load and parse APL file."""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)

            if not isinstance(data, list):
                self.logger.error("APL file does not contain a list")
                return None

            return data

        except json.JSONDecodeError as e:
            self.logger.error(f"Invalid JSON in APL file: {e}")
            return None
        except FileNotFoundError:
            self.logger.error(f"APL file not found: {file_path}")
            return None
        except Exception as e:
            self.logger.error(f"Error loading APL file: {e}")
            return None

    def _validate_apl_data(self, apl_data: List[Dict]) -> Dict[str, Any]:
        """
        Validate APL data structure.

        Returns:
            Dictionary with 'valid' boolean and 'errors' list
        """
        errors = []

        for index, entry in enumerate(apl_data):
            if not isinstance(entry, dict):
                errors.append(f"Entry {index}: Not a dictionary")
                continue

            # Check required fields
            if 'asset_id' not in entry:
                errors.append(f"Entry {index}: Missing 'asset_id' field")

            if 'fields' not in entry:
                errors.append(f"Entry {index}: Missing 'fields' field")
            elif not isinstance(entry['fields'], dict):
                errors.append(f"Entry {index}: 'fields' must be a dictionary")
            elif not entry['fields']:
                errors.append(f"Entry {index}: 'fields' dictionary is empty")

        return {
            'valid': len(errors) == 0,
            'errors': errors
        }

    def _prepare_rollback(self, apl_data: List[Dict]):
        """Prepare rollback data by fetching current asset states."""
        self.logger.info("Preparing rollback data...")
        self.rollback_data = []

        for entry in apl_data:
            asset_id = entry.get('asset_id')
            if not asset_id:
                continue

            # Get current state of the asset
            current_state = self._get_asset_current_state(asset_id)
            if current_state:
                self.rollback_data.append({
                    'asset_id': asset_id,
                    'original_fields': current_state
                })

    def _get_asset_current_state(self, asset_id: str) -> Optional[Dict]:
        """Fetch current state of an asset from Topdesk."""
        if self.dry_run:
            return {}

        try:
            if self.use_api_directly:
                return self._get_asset_via_api(asset_id)

            cmd = self._build_cli_base_command()
            cmd.extend(['asset', 'get', asset_id, '--format', 'json'])

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30,
                env=self._get_cli_env()
            )

            if result.returncode == 0:
                return json.loads(result.stdout)
            else:
                self.logger.warning(f"Could not fetch current state for asset {asset_id}")
                return None

        except subprocess.TimeoutExpired:
            self.logger.error(f"Timeout fetching asset {asset_id}")
            return None
        except json.JSONDecodeError:
            self.logger.error(f"Invalid JSON response for asset {asset_id}")
            return None
        except Exception as e:
            self.logger.error(f"Error fetching asset {asset_id}: {e}")
            return None

    def _get_asset_via_api(self, asset_id: str) -> Optional[Dict]:
        """Fetch asset directly via API."""
        try:
            import requests

            auth_string = f"{self.topdesk_username}:{self.topdesk_api_key}"
            auth_bytes = auth_string.encode('ascii')
            auth_b64 = base64.b64encode(auth_bytes).decode('ascii')

            headers = {
                'Authorization': f'Basic {auth_b64}',
                'Content-Type': 'application/json'
            }

            response = requests.get(
                f"{self.topdesk_url}/tas/api/assetmgmt/assets/{asset_id}",
                headers=headers,
                timeout=30
            )

            if response.status_code == 200:
                return response.json()
            elif response.status_code == 404:
                self.logger.warning(f"Asset not found: {asset_id}")
                return None
            else:
                self.logger.error(f"API error fetching asset {asset_id}: {response.status_code}")
                return None

        except Exception as e:
            self.logger.error(f"Error fetching asset via API: {e}")
            return None

    def _process_assets_batch(self, apl_data: List[Dict]) -> List[Dict]:
        """Process assets in batches."""
        results = []

        # Process in batches
        for i in range(0, len(apl_data), self.batch_size):
            batch = apl_data[i:i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (len(apl_data) + self.batch_size - 1) // self.batch_size

            self.logger.info(f"Processing batch {batch_num}/{total_batches} ({len(batch)} assets)")

            for entry in batch:
                result = self._process_single_asset(entry)
                results.append(result)

                # Update statistics
                if result['status'] == 'success':
                    self.stats['successful_updates'] += 1
                elif result['status'] == 'partial':
                    self.stats['partial_updates'] += 1
                elif result['status'] == 'failed':
                    self.stats['failed_updates'] += 1
                elif result['status'] == 'skipped':
                    self.stats['skipped'] += 1

            # Small delay between batches to avoid overwhelming the API
            if i + self.batch_size < len(apl_data):
                time.sleep(0.5)

        return results

    def _process_single_asset(self, entry: Dict) -> Dict:
        """Process a single asset update."""
        asset_id = entry.get('asset_id')
        fields = entry.get('fields', {})

        if not asset_id:
            return {
                'asset_id': 'unknown',
                'status': 'failed',
                'error': 'Missing asset_id'
            }

        self.logger.debug(f"Processing asset {asset_id} with {len(fields)} field(s)")

        if self.dry_run:
            self.logger.info(f"[DRY RUN] Would update asset {asset_id} with fields: {fields}")
            return {
                'asset_id': asset_id,
                'status': 'success',
                'fields_updated': list(fields.keys()),
                'dry_run': True
            }

        # Execute with retries
        for attempt in range(self.max_retries):
            try:
                if self.use_api_directly:
                    success, error_msg = self._update_asset_via_api(asset_id, fields)
                    if success:
                        self.logger.info(f"Successfully updated asset {asset_id}")
                        return {
                            'asset_id': asset_id,
                            'status': 'success',
                            'fields_updated': list(fields.keys())
                        }
                    else:
                        # Check if it's a retryable error
                        if self._is_retryable_error(error_msg) and attempt < self.max_retries - 1:
                            wait_time = self.retry_delay * (2 ** attempt)
                            self.logger.warning(f"Retryable error for {asset_id}, waiting {wait_time}s...")
                            time.sleep(wait_time)
                            continue

                        # Try field-by-field update if batch failed
                        if len(fields) > 1 and attempt == self.max_retries - 1:
                            return self._process_fields_individually(asset_id, fields)

                        self.logger.error(f"Failed to update asset {asset_id}: {error_msg}")
                        return {
                            'asset_id': asset_id,
                            'status': 'failed',
                            'error': error_msg
                        }
                else:
                    # Build command
                    cmd = self._build_update_command(asset_id, fields)

                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=60,
                        env=self._get_cli_env()
                    )

                if result.returncode == 0:
                    self.logger.info(f"Successfully updated asset {asset_id}")
                    return {
                        'asset_id': asset_id,
                        'status': 'success',
                        'fields_updated': list(fields.keys())
                    }
                else:
                    error_msg = result.stderr or result.stdout

                    # Handle specific error codes
                    error_code = self._extract_error_code(error_msg)
                    if error_code:
                        if error_code == 401:
                            self.logger.error(f"Authentication failed for asset {asset_id}")
                            self._validate_authentication()  # Re-validate auth
                        elif error_code == 404:
                            self.logger.error(f"Asset not found: {asset_id}")
                            return {
                                'asset_id': asset_id,
                                'status': 'failed',
                                'error': 'Asset not found'
                            }
                        elif error_code == 429:
                            # Rate limiting
                            retry_after = self._extract_retry_after(error_msg) or 60
                            self.logger.warning(f"Rate limited, waiting {retry_after}s...")
                            time.sleep(retry_after)
                            continue

                    # Check if it's a retryable error
                    if self._is_retryable_error(error_msg) and attempt < self.max_retries - 1:
                        wait_time = self.retry_delay * (2 ** attempt)
                        self.logger.warning(f"Retryable error for {asset_id}, waiting {wait_time}s...")
                        time.sleep(wait_time)
                        continue

                    # Try field-by-field update if batch failed
                    if len(fields) > 1 and attempt == self.max_retries - 1:
                        return self._process_fields_individually(asset_id, fields)

                    self.logger.error(f"Failed to update asset {asset_id}: {error_msg}")
                    return {
                        'asset_id': asset_id,
                        'status': 'failed',
                        'error': error_msg
                    }

            except subprocess.TimeoutExpired:
                self.logger.error(f"Timeout updating asset {asset_id}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay * (2 ** attempt))
                    continue
                return {
                    'asset_id': asset_id,
                    'status': 'failed',
                    'error': 'Operation timeout'
                }
            except Exception as e:
                self.logger.error(f"Unexpected error updating asset {asset_id}: {e}")
                return {
                    'asset_id': asset_id,
                    'status': 'failed',
                    'error': str(e)
                }

        return {
            'asset_id': asset_id,
            'status': 'failed',
            'error': 'Max retries exceeded'
        }

    def _build_update_command(self, asset_id: str, fields: Dict) -> List[str]:
        """Build topdesk-cli update command."""
        cmd = self._build_cli_base_command()

        # Try different command formats based on CLI version
        # Format 1: topdesk asset update <asset_id> --field "name=value"
        # Format 2: topdesk-cli asset update --id <asset_id> --field name=value
        # Format 3: topdesk asset patch <asset_id> with JSON payload

        if 'asset' not in cmd:
            cmd.append('asset')

        # Try to determine the best format
        if self.cli_available:
            cmd.extend(['update', '--id', asset_id])
        else:
            cmd.extend(['update', asset_id])

        for field_name, field_value in fields.items():
            # Handle different value types
            if field_value is None:
                value_str = ''
            elif isinstance(field_value, bool):
                value_str = 'true' if field_value else 'false'
            elif isinstance(field_value, (int, float)):
                value_str = str(field_value)
            else:
                value_str = str(field_value)

            # Try both quoted and unquoted formats
            if ' ' in value_str or '"' in value_str:
                # Quote the entire field=value pair if it contains spaces
                cmd.extend(['--field', f'"{field_name}={value_str}"'])
            else:
                cmd.extend(['--field', f'{field_name}={value_str}'])

        return cmd

    def _update_asset_via_api(self, asset_id: str, fields: Dict) -> Tuple[bool, str]:
        """Update asset directly via API."""
        try:
            import requests

            auth_string = f"{self.topdesk_username}:{self.topdesk_api_key}"
            auth_bytes = auth_string.encode('ascii')
            auth_b64 = base64.b64encode(auth_bytes).decode('ascii')

            headers = {
                'Authorization': f'Basic {auth_b64}',
                'Content-Type': 'application/json'
            }

            # Transform fields to API format
            api_payload = self._transform_fields_for_api(fields)

            response = requests.patch(
                f"{self.topdesk_url}/tas/api/assetmgmt/assets/{asset_id}",
                headers=headers,
                json=api_payload,
                timeout=60
            )

            if response.status_code in [200, 204]:
                return True, "Success"
            elif response.status_code == 401:
                return False, "Authentication failed (401)"
            elif response.status_code == 404:
                return False, "Asset not found (404)"
            elif response.status_code == 429:
                retry_after = response.headers.get('Retry-After', '60')
                return False, f"Rate limited (429). Retry after {retry_after}s"
            elif response.status_code >= 500:
                return False, f"Server error ({response.status_code})"
            else:
                error_detail = response.text[:200] if response.text else "No details"
                return False, f"API error ({response.status_code}): {error_detail}"

        except requests.exceptions.Timeout:
            return False, "Request timeout"
        except requests.exceptions.ConnectionError:
            return False, "Connection error"
        except Exception as e:
            return False, f"Unexpected error: {str(e)}"

    def _transform_fields_for_api(self, fields: Dict) -> Dict:
        """Transform field names and values for Topdesk API format."""
        # Map common field names to Topdesk API field names
        field_mapping = {
            'ip_address': 'ipAddress',
            'mac_address': 'macAddress',
            'serial_number': 'serialNumber',
            'asset_tag': 'assetTag',
            'operating_system': 'operatingSystem'
        }

        api_fields = {}
        for field_name, field_value in fields.items():
            # Map field name if needed
            api_field_name = field_mapping.get(field_name, field_name)

            # Handle special value types
            if field_value is None:
                api_fields[api_field_name] = None
            elif isinstance(field_value, bool):
                api_fields[api_field_name] = field_value
            elif isinstance(field_value, (int, float)):
                api_fields[api_field_name] = field_value
            else:
                api_fields[api_field_name] = str(field_value)

        return api_fields

    def _process_fields_individually(self, asset_id: str, fields: Dict) -> Dict:
        """Process each field individually if batch update fails."""
        self.logger.info(f"Attempting field-by-field update for asset {asset_id}")

        successful_fields = []
        failed_fields = []

        for field_name, field_value in fields.items():
            if self.use_api_directly:
                success, error_msg = self._update_asset_via_api(asset_id, {field_name: field_value})
                if success:
                    successful_fields.append(field_name)
                    self.logger.debug(f"Successfully updated field {field_name} for asset {asset_id}")
                else:
                    failed_fields.append({
                        'field': field_name,
                        'error': error_msg
                    })
                    self.logger.warning(f"Failed to update field {field_name} for asset {asset_id}: {error_msg}")
            else:
                cmd = self._build_update_command(asset_id, {field_name: field_value})

                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=30,
                        env=self._get_cli_env()
                    )

                    if result.returncode == 0:
                        successful_fields.append(field_name)
                        self.logger.debug(f"Successfully updated field {field_name} for asset {asset_id}")
                    else:
                        failed_fields.append({
                            'field': field_name,
                            'error': result.stderr or result.stdout
                        })
                        self.logger.warning(f"Failed to update field {field_name} for asset {asset_id}")

                except Exception as e:
                    failed_fields.append({
                        'field': field_name,
                        'error': str(e)
                    })

        if successful_fields and not failed_fields:
            status = 'success'
        elif successful_fields and failed_fields:
            status = 'partial'
        else:
            status = 'failed'

        return {
            'asset_id': asset_id,
            'status': status,
            'fields_updated': successful_fields,
            'fields_failed': failed_fields
        }

    def _is_retryable_error(self, error_msg: str) -> bool:
        """Check if error is retryable."""
        retryable_patterns = [
            'connection timeout',
            'connection refused',
            'temporarily unavailable',
            'rate limit',
            'too many requests',
            '429',  # Rate limiting
            '502',  # Bad gateway
            '503',  # Service unavailable
            '504',  # Gateway timeout
            'gateway timeout',
            'server error',
            'socket',
            'network'
        ]

        error_lower = error_msg.lower()
        return any(pattern in error_lower for pattern in retryable_patterns)

    def _extract_error_code(self, error_msg: str) -> Optional[int]:
        """Extract HTTP error code from error message."""
        import re

        # Look for patterns like "401", "(401)", "status 401", "error 401"
        patterns = [
            r'\b(\d{3})\b',  # Just the number
            r'status[:\s]+(\d{3})',
            r'error[:\s]+(\d{3})',
            r'code[:\s]+(\d{3})'
        ]

        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue

        return None

    def _extract_retry_after(self, error_msg: str) -> Optional[int]:
        """Extract Retry-After value from error message."""
        import re

        # Look for patterns like "retry after 60", "Retry-After: 60"
        patterns = [
            r'retry[\s-]after[:\s]+(\d+)',
            r'wait[:\s]+(\d+)[\s]*(?:seconds?)?',
            r'(\d+)[\s]*seconds?[\s]*(?:before|until)'
        ]

        for pattern in patterns:
            match = re.search(pattern, error_msg, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    continue

        return None

    def _perform_rollback(self):
        """Perform rollback of applied changes."""
        self.logger.warning("Performing rollback of applied changes...")

        rollback_success = 0
        rollback_failed = 0

        for item in self.rollback_data:
            asset_id = item['asset_id']
            original_fields = item['original_fields']

            if not original_fields:
                continue

            if self.use_api_directly:
                success, error_msg = self._update_asset_via_api(asset_id, original_fields)
                if success:
                    rollback_success += 1
                    self.logger.info(f"Successfully rolled back asset {asset_id}")
                else:
                    rollback_failed += 1
                    self.logger.error(f"Failed to rollback asset {asset_id}: {error_msg}")
            else:
                cmd = self._build_update_command(asset_id, original_fields)

                try:
                    result = subprocess.run(
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=60,
                        env=self._get_cli_env()
                    )

                    if result.returncode == 0:
                        rollback_success += 1
                        self.logger.info(f"Successfully rolled back asset {asset_id}")
                    else:
                        rollback_failed += 1
                        self.logger.error(f"Failed to rollback asset {asset_id}")

                except Exception as e:
                    rollback_failed += 1
                    self.logger.error(f"Error during rollback of asset {asset_id}: {e}")

        self.stats['rollbacks'] = rollback_success

        # Save rollback results
        rollback_file = self.output_dir / 'apply' / 'rollback' / f'rollback_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(rollback_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'success': rollback_success,
                'failed': rollback_failed,
                'data': self.rollback_data
            }, f, indent=2)

    def _generate_report(self, apl_file_path: str, results: List[Dict], start_time: float) -> Path:
        """Generate processing report."""
        execution_time = time.time() - start_time

        report = {
            'timestamp': datetime.now().isoformat(),
            'apl_file': apl_file_path,
            'execution_time_seconds': round(execution_time, 2),
            'dry_run': self.dry_run,
            'statistics': self.stats,
            'results': results
        }

        # Save JSON report
        report_filename = f'report_{Path(apl_file_path).stem}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        report_path = self.output_dir / 'apply' / 'reports' / report_filename

        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)

        # Generate text summary
        summary = self._generate_text_summary(report)
        summary_path = report_path.with_suffix('.txt')

        with open(summary_path, 'w') as f:
            f.write(summary)

        # Print summary to console
        print(summary)

        return report_path

    def _generate_text_summary(self, report: Dict) -> str:
        """Generate human-readable text summary."""
        stats = report['statistics']

        summary = []
        summary.append("Application Summary")
        summary.append("=" * 50)
        summary.append(f"APL File: {Path(report['apl_file']).name}")
        summary.append(f"Timestamp: {report['timestamp']}")
        summary.append(f"Execution Time: {report['execution_time_seconds']}s")

        if report['dry_run']:
            summary.append("\n** DRY RUN MODE - No actual changes applied **")

        summary.append("")
        summary.append("Statistics:")
        summary.append(f"  Total Assets: {stats['total_assets']}")
        summary.append(f"  Successfully Updated: {stats['successful_updates']}")
        summary.append(f"  Failed Updates: {stats['failed_updates']}")
        summary.append(f"  Partially Updated: {stats['partial_updates']}")
        summary.append(f"  Skipped: {stats['skipped']}")

        if stats['rollbacks'] > 0:
            summary.append(f"  Rollbacks Performed: {stats['rollbacks']}")

        # Add failed asset details
        failed_assets = [r for r in report['results'] if r['status'] == 'failed']
        if failed_assets:
            summary.append("")
            summary.append("Failed Assets:")
            for asset in failed_assets[:10]:  # Show first 10 failures
                error = asset.get('error', 'Unknown error')
                summary.append(f"  - {asset['asset_id']}: {error[:80]}")

            if len(failed_assets) > 10:
                summary.append(f"  ... and {len(failed_assets) - 10} more")

        # Add partial update details
        partial_assets = [r for r in report['results'] if r['status'] == 'partial']
        if partial_assets:
            summary.append("")
            summary.append("Partially Updated Assets:")
            for asset in partial_assets[:5]:
                summary.append(f"  - {asset['asset_id']}")
                if 'fields_updated' in asset:
                    summary.append(f"    Updated: {', '.join(asset['fields_updated'])}")
                if 'fields_failed' in asset:
                    failed = [f['field'] for f in asset['fields_failed']]
                    summary.append(f"    Failed: {', '.join(failed)}")

        return "\n".join(summary)

    def _archive_apl_file(self, apl_file_path: str, success: bool):
        """Archive processed APL file."""
        source_path = Path(apl_file_path)

        if not source_path.exists():
            return

        # Determine destination directory
        if success:
            dest_dir = self.output_dir / 'apply' / 'success'
        else:
            dest_dir = self.output_dir / 'apply' / 'failed'

        # Create timestamped filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        dest_filename = f"{source_path.stem}_{timestamp}{source_path.suffix}"
        dest_path = dest_dir / dest_filename

        try:
            shutil.copy2(source_path, dest_path)
            self.logger.info(f"Archived APL file to {dest_path}")
        except Exception as e:
            self.logger.error(f"Failed to archive APL file: {e}")

    def _create_result(self, success: bool, **kwargs) -> Dict[str, Any]:
        """Create standardized result dictionary."""
        result = {
            'success': success,
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats.copy()
        }
        result.update(kwargs)
        return result


class BatchProcessor:
    """Handle batch processing of multiple APL files."""

    def __init__(self, processor: APLProcessor):
        """
        Initialize batch processor.

        Args:
            processor: APLProcessor instance to use
        """
        self.processor = processor
        self.results = []

    def process_directory(self, directory: str, pattern: str = "*.apl") -> Dict[str, Any]:
        """
        Process all APL files in a directory.

        Args:
            directory: Directory containing APL files
            pattern: File pattern to match (default: *.apl)

        Returns:
            Summary of all processing results
        """
        directory_path = Path(directory)

        if not directory_path.exists():
            return {
                'success': False,
                'error': f"Directory does not exist: {directory}"
            }

        # Find all matching files
        apl_files = list(directory_path.glob(pattern))

        if not apl_files:
            return {
                'success': False,
                'error': f"No APL files found matching pattern: {pattern}"
            }

        print(f"Found {len(apl_files)} APL file(s) to process")

        # Process each file
        for apl_file in apl_files:
            print(f"\nProcessing: {apl_file.name}")
            result = self.processor.process_apl_file(str(apl_file))

            self.results.append({
                'file': str(apl_file),
                'result': result
            })

        # Generate overall summary
        return self._generate_batch_summary()

    def _generate_batch_summary(self) -> Dict[str, Any]:
        """Generate summary of batch processing results."""
        total_stats = defaultdict(int)
        successful_files = 0
        failed_files = 0

        for item in self.results:
            result = item['result']

            if result['success']:
                successful_files += 1
            else:
                failed_files += 1

            # Aggregate statistics
            if 'stats' in result:
                for key, value in result['stats'].items():
                    total_stats[key] += value

        return {
            'success': failed_files == 0,
            'files_processed': len(self.results),
            'files_successful': successful_files,
            'files_failed': failed_files,
            'total_statistics': dict(total_stats),
            'details': self.results
        }


def main():
    """Main entry point for the apply module."""
    import argparse

    parser = argparse.ArgumentParser(
        description='Apply APL files to update Topdesk assets'
    )

    parser.add_argument(
        'input',
        help='APL file or directory containing APL files'
    )

    parser.add_argument(
        '--output-dir',
        default='./output',
        help='Output directory for reports and logs (default: ./output)'
    )

    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='Number of assets to process in a batch (default: 10)'
    )

    parser.add_argument(
        '--max-retries',
        type=int,
        default=3,
        help='Maximum retry attempts for failed operations (default: 3)'
    )

    parser.add_argument(
        '--retry-delay',
        type=float,
        default=2.0,
        help='Initial delay between retries in seconds (default: 2.0)'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Simulate changes without applying them'
    )

    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )

    parser.add_argument(
        '--parallel',
        action='store_true',
        help='Enable parallel processing'
    )

    parser.add_argument(
        '--parallel-workers',
        type=int,
        default=4,
        help='Number of parallel workers (default: 4)'
    )

    parser.add_argument(
        '--pattern',
        default='*.apl',
        help='File pattern when processing directory (default: *.apl)'
    )

    args = parser.parse_args()

    # Create processor
    processor = APLProcessor(
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay,
        dry_run=args.dry_run,
        verbose=args.verbose,
        parallel=args.parallel,
        parallel_workers=args.parallel_workers
    )

    input_path = Path(args.input)

    if input_path.is_file():
        # Process single file
        if not str(input_path).endswith('.apl'):
            print(f"Warning: File does not have .apl extension: {input_path}")

        result = processor.process_apl_file(str(input_path))

        if result['success']:
            print(f"\nProcessing completed successfully")
            sys.exit(0)
        else:
            print(f"\nProcessing failed: {result.get('error', 'Unknown error')}")
            sys.exit(1)

    elif input_path.is_dir():
        # Process directory
        batch_processor = BatchProcessor(processor)
        result = batch_processor.process_directory(str(input_path), args.pattern)

        # Print summary
        print("\n" + "=" * 50)
        print("Batch Processing Summary")
        print("=" * 50)
        print(f"Files Processed: {result['files_processed']}")
        print(f"Files Successful: {result['files_successful']}")
        print(f"Files Failed: {result['files_failed']}")

        if 'total_statistics' in result:
            print("\nTotal Statistics:")
            for key, value in result['total_statistics'].items():
                print(f"  {key}: {value}")

        if result['success']:
            sys.exit(0)
        else:
            sys.exit(1)

    else:
        print(f"Error: Input path does not exist: {input_path}")
        sys.exit(1)


if __name__ == '__main__':
    main()